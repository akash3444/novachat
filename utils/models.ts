export const models = [
  // Google Generative AI
  {
    id: "google/gemini-2.5-flash-preview-05-20",
    hugging_face_id: "",
    name: "Gemini 2.5 Flash",
    created: 1747761924,
    description:
      'Gemini 2.5 Flash May 20th Checkpoint is Google\'s state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in "thinking" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nNote: This model is available in two variants: thinking and non-thinking. The output pricing varies significantly depending on whether the thinking capability is active. If you select the standard variant (without the ":thinking" suffix), the model will explicitly avoid generating thinking tokens. \n\nTo utilize the thinking capability and receive thinking tokens, you must choose the ":thinking" variant, which will then incur the higher thinking-output pricing. \n\nAdditionally, Gemini 2.5 Flash is configurable through the "max tokens for reasoning" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).',
    context_length: 1048576,
    architecture: {
      modality: "text+image->text",
      input_modalities: ["image", "text", "file"],
      output_modalities: ["text"],
      tokenizer: "Gemini",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.00000015",
      completion: "0.0000006",
      request: "0",
      image: "0.0006192",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.0000000375",
      input_cache_write: "0.0000002333",
    },
    top_provider: {
      context_length: 1048576,
      max_completion_tokens: 65535,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "tools",
      "tool_choice",
      "max_tokens",
      "temperature",
      "top_p",
      "reasoning",
      "include_reasoning",
      "structured_outputs",
      "response_format",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "seed",
    ],
  },
  {
    id: "google/gemini-2.5-pro-preview",
    hugging_face_id: "",
    name: "Gemini 2.5 Pro",
    created: 1749137257,
    description:
      "Gemini 2.5 Pro is Google's state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.\n",
    context_length: 1048576,
    architecture: {
      modality: "text+image->text",
      input_modalities: ["file", "image", "text"],
      output_modalities: ["text"],
      tokenizer: "Gemini",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.00000125",
      completion: "0.00001",
      request: "0",
      image: "0.00516",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.00000031",
      input_cache_write: "0.000001625",
    },
    top_provider: {
      context_length: 1048576,
      max_completion_tokens: 65536,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "tools",
      "tool_choice",
      "max_tokens",
      "temperature",
      "top_p",
      "reasoning",
      "include_reasoning",
      "structured_outputs",
      "response_format",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "seed",
    ],
  },
  // OpenAI
  {
    id: "openai/o4-mini",
    hugging_face_id: "",
    name: "o4 Mini",
    created: 1744820942,
    description:
      "OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.",
    context_length: 200000,
    architecture: {
      modality: "text+image->text",
      input_modalities: ["image", "text"],
      output_modalities: ["text"],
      tokenizer: "Other",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.0000011",
      completion: "0.0000044",
      request: "0",
      image: "0.0008415",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.000000275",
    },
    top_provider: {
      context_length: 200000,
      max_completion_tokens: 100000,
      is_moderated: true,
    },
    per_request_limits: null,
    supported_parameters: [
      "tools",
      "tool_choice",
      "seed",
      "max_tokens",
      "response_format",
      "structured_outputs",
    ],
  },
  {
    id: "openai/gpt-4o-mini",
    hugging_face_id: null,
    name: "GPT 4o-mini",
    created: 1721260800,
    description:
      "GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal",
    context_length: 128000,
    architecture: {
      modality: "text+image->text",
      input_modalities: ["text", "image", "file"],
      output_modalities: ["text"],
      tokenizer: "GPT",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.00000015",
      completion: "0.0000006",
      request: "0",
      image: "0.000217",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.000000075",
    },
    top_provider: {
      context_length: 128000,
      max_completion_tokens: 16384,
      is_moderated: true,
    },
    per_request_limits: null,
    supported_parameters: [
      "max_tokens",
      "temperature",
      "top_p",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "web_search_options",
      "seed",
      "logit_bias",
      "logprobs",
      "top_logprobs",
      "response_format",
      "structured_outputs",
      "tools",
      "tool_choice",
    ],
  },
  // Claude
  {
    id: "anthropic/claude-3.7-sonnet",
    hugging_face_id: "",
    name: "Claude 3.7 Sonnet",
    created: 1740422110,
    description:
      "Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)",
    context_length: 200000,
    architecture: {
      modality: "text+image->text",
      input_modalities: ["text", "image"],
      output_modalities: ["text"],
      tokenizer: "Claude",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.000003",
      completion: "0.000015",
      request: "0",
      image: "0.0048",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.0000003",
      input_cache_write: "0.00000375",
    },
    top_provider: {
      context_length: 200000,
      max_completion_tokens: 64000,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "max_tokens",
      "temperature",
      "stop",
      "reasoning",
      "include_reasoning",
      "tools",
      "tool_choice",
      "top_p",
      "top_k",
    ],
  },
  {
    id: "anthropic/claude-3.7-sonnet:thinking",
    hugging_face_id: "",
    name: "Claude 3.7 Sonnet (thinking)",
    created: 1740422110,
    description:
      "Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)",
    context_length: 200000,
    architecture: {
      modality: "text+image->text",
      input_modalities: ["text", "image"],
      output_modalities: ["text"],
      tokenizer: "Claude",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.000003",
      completion: "0.000015",
      request: "0",
      image: "0.0048",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.0000003",
      input_cache_write: "0.00000375",
    },
    top_provider: {
      context_length: 200000,
      max_completion_tokens: 128000,
      is_moderated: true,
    },
    per_request_limits: null,
    supported_parameters: [
      "max_tokens",
      "temperature",
      "stop",
      "reasoning",
      "include_reasoning",
      "tools",
      "tool_choice",
    ],
  },
  // xAI
  {
    id: "x-ai/grok-3-beta",
    hugging_face_id: "",
    name: "Grok 3 Beta",
    created: 1744240068,
    description:
      "Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.\n\nExcels in structured tasks and benchmarks like GPQA, LCB, and MMLU-Pro where it outperforms Grok 3 Mini even on high thinking. \n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n",
    context_length: 131072,
    architecture: {
      modality: "text->text",
      input_modalities: ["text"],
      output_modalities: ["text"],
      tokenizer: "Grok",
      instruct_type: null,
    },
    pricing: {
      prompt: "0.000003",
      completion: "0.000015",
      request: "0",
      image: "0",
      web_search: "0",
      internal_reasoning: "0",
      input_cache_read: "0.00000075",
    },
    top_provider: {
      context_length: 131072,
      max_completion_tokens: null,
      is_moderated: false,
    },
    per_request_limits: null,
    supported_parameters: [
      "tools",
      "tool_choice",
      "max_tokens",
      "temperature",
      "top_p",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "seed",
      "logprobs",
      "top_logprobs",
      "response_format",
    ],
  },
];

export const DEFAULT_MODEL = "openai/gpt-4o-mini";
